{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset v2 albumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.path.dirname(os.path.dirname(os.path.abspath(__file__)))='/data/ylw/code/yolo/yolox-pytorch/yolox-v3/tools'\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "os.chdir(os.path.dirname(os.path.abspath('.')))\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# import dataset.coco_dataset_yolox_v1 as DS\n",
    "import dataset.coco_dataset_yolox_v2 as DS\n",
    "import configs.config_instance as Config\n",
    "from tools.viz.pil_draw import PILDraw\n",
    "from IPython import display\n",
    "import cv2\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "# %matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(DS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "load train ann-file: /data/ylw/dataset/voc2coco/annotations/instances_val.json / 1000\n"
     ]
    }
   ],
   "source": [
    "cfgs = Config.TRAINCONFIGS  # EVALCONFIGS, TRAINCONFIGS\n",
    "cfgs.dataset_root = r'/data/ylw/dataset/voc2coco'\n",
    "# ann_file = os.path.join(cfgs.dataset_root, 'annotations', f'person_keypoints_{cfgs.dataset_set_name}.json')\n",
    "ann_file = os.path.join(cfgs.dataset_root, 'annotations', f'instances_val.json')\n",
    "coco = COCO(ann_file)\n",
    "img_ids = coco.getImgIds()\n",
    "print(f'load train ann-file: {ann_file} / {len(img_ids)}')\n",
    "\n",
    "\n",
    "ds = DS.COCODetDataset(\n",
    "    root_dir=cfgs.dataset_root, \n",
    "    input_shape=cfgs.input_shape, \n",
    "    mosaic=cfgs.mosaic, mosaic_prob=cfgs.mosaic_prob, \n",
    "    mixup=cfgs.mixup, mixup_prob=cfgs.mixup_prob, \n",
    "    train=True, \n",
    "    set_name='val', \n",
    "    coco=coco,\n",
    "    special_aug_ratio = 0.7,\n",
    "    mixup_alpha=0.5, mixup_beta=0.5, \n",
    "    jitter=True, random_scale=cfgs.random_scale,\n",
    "    ex_kpts=0,\n",
    "    class_list=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    ds,\n",
    "    shuffle = False,\n",
    "    batch_size = 2,\n",
    "    num_workers = cfgs.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=ds.yolox_dataset_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([12, 5])\n",
      "[0]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([10, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([13, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([3, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([10, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([27, 5])\n",
      "[5]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([2, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([2, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([4, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([7, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "[10]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([2, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([2, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([2, 5])\n",
      "[15]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([11, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([20, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([4, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([6, 5])\n",
      "[20]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([5, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([2, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([15, 5])\n",
      "[25]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([19, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([3, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([10, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([24, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([8, 5])\n",
      "[30]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([11, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([3, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([2, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([3, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([21, 5])\n",
      "[35]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([6, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([4, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([6, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([8, 5])\n",
      "[40]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([4, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([5, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([8, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([10, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([6, 5])\n",
      "[45]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([5, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([21, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([13, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([14, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "[50]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([21, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([5, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([3, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([2, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([10, 5])\n",
      "[55]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([3, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([12, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([11, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([11, 5])\n",
      "[60]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([4, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([12, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([4, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([14, 5])\n",
      "[65]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([2, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([1, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([15, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([21, 5])\n",
      "[70]  imgs_.shape=(640, 1280, 3)\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([10, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([19, 5])\n",
      "imgs.shape=torch.Size([2, 3, 640, 640])  len(bboxes)=2  bboxes[0].shape=torch.Size([12, 5])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch_data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dl):\n\u001b[1;32m      2\u001b[0m     imgs, bboxes \u001b[39m=\u001b[39m batch_data[\u001b[39m0\u001b[39m], batch_data[\u001b[39m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mimgs\u001b[39m.\u001b[39mshape\u001b[39m=}\u001b[39;00m\u001b[39m  \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(bboxes)\u001b[39m=}\u001b[39;00m\u001b[39m  \u001b[39m\u001b[39m{\u001b[39;00mbboxes[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\u001b[39m=}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/train/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/train/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/train/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1272\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m   1271\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m-> 1272\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1273\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1274\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/train/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/train/lib/python3.9/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[1;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/anaconda3/envs/train/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch_idx, batch_data in enumerate(dl):\n",
    "    imgs, bboxes = batch_data[0], batch_data[1]\n",
    "    print(f'{imgs.shape=}  {len(bboxes)=}  {bboxes[0].shape=}')\n",
    "    # if batch_idx > 10:\n",
    "    #     break\n",
    "    imgs_ = []\n",
    "    for data_idx in range(batch_data[0].shape[0]):\n",
    "        img, bbox, label = imgs[data_idx, ...].cpu().numpy(), bboxes[data_idx][:, 0:4].cpu().numpy(), bboxes[data_idx][:, 4].cpu().numpy()\n",
    "        # print(f'[{batch_idx}/{data_idx}][0]  {bbox[:10, ...]=}')\n",
    "        # bbox = np.array([box for box in bbox.numpy() if all(box > 0)])\n",
    "        # print(f'[{batch_idx}/{data_idx}][1]  {bbox[:10, ...]=}')\n",
    "        img = (np.transpose(img, (1,2,0))*128+127.5).astype(np.uint8)\n",
    "        bbox[:, 0] -= bbox[:, 2]/2\n",
    "        bbox[:, 1] -= bbox[:, 3]/2\n",
    "        bbox[:, 2] += bbox[:, 0]\n",
    "        bbox[:, 3] += bbox[:, 1]\n",
    "        bbox = bbox.astype(np.uint32)\n",
    "        img = PILDraw(font_file=r'./tools/viz/simhei.ttf').rectangle(Image.fromarray(img), bbox, [{'cls': int(l)} for l in label])\n",
    "        imgs_.append(img)\n",
    "    imgs_ = np.concatenate(imgs_, axis=1)\n",
    "    Image.fromarray(imgs_).save(f'./test/tmp_data/{batch_idx:08}-{data_idx:08}.jpg')\n",
    "    if batch_idx % 5 == 0:\n",
    "        print(f'[{batch_idx}]  {imgs_.shape=}')\n",
    "    # plt.cla()\n",
    "    # plt.imshow(imgs_);\n",
    "    # display.clear_output(wait=True)\n",
    "    # plt.pause(0.001)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ori dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "os.chdir(os.path.dirname(os.path.abspath('.')))\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import dataset.coco_dataset_yolox as DS\n",
    "import configs.config_instance as Config\n",
    "from tools.viz.pil_draw import PILDraw\n",
    "from IPython import display\n",
    "import cv2\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "# %matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = Config.TRAINCONFIGS  # EVALCONFIGS, TRAINCONFIGS\n",
    "cfgs.dataset_root = r'/data/ylw/dataset/voc2coco'\n",
    "# ann_file = os.path.join(cfgs.dataset_root, 'annotations', f'person_keypoints_{cfgs.dataset_set_name}.json')\n",
    "ann_file = os.path.join(cfgs.dataset_root, 'annotations', f'instances_val.json')\n",
    "coco = COCO(ann_file)\n",
    "img_ids = coco.getImgIds()\n",
    "print(f'load train ann-file: {ann_file} / {len(img_ids)}')\n",
    "\n",
    "\n",
    "ds = DS.COCODetDataset(\n",
    "    root_dir=cfgs.dataset_root, \n",
    "    input_shape=cfgs.input_shape, \n",
    "    num_classes=cfgs.num_classes, \n",
    "    mosaic=cfgs.mosaic, mosaic_prob=cfgs.mosaic_prob, \n",
    "    mixup=cfgs.mixup, mixup_prob=cfgs.mixup_prob, \n",
    "    train=True, \n",
    "    set_name='val', \n",
    "    coco=coco,\n",
    "    special_aug_ratio = 0.7,\n",
    "    mixup_alpha=0.5, mixup_beta=0.5, \n",
    "    ex_kpts=0,\n",
    "    class_list=None\n",
    ")\n",
    "dl = DataLoader(\n",
    "    ds,\n",
    "    shuffle = False,\n",
    "    batch_size = 2,\n",
    "    num_workers = cfgs.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=ds.yolox_dataset_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch_data in enumerate(dl):\n",
    "    imgs, bboxes = batch_data[0], batch_data[1]\n",
    "    print(f'{imgs.shape=}  {len(bboxes)=}  {bboxes[0].shape=}')\n",
    "    # if batch_idx > 10:\n",
    "    #     break\n",
    "    imgs_ = []\n",
    "    for data_idx in range(batch_data[0].shape[0]):\n",
    "        img, bbox, label = imgs[data_idx, ...].cpu().numpy(), bboxes[data_idx][:, 0:4].cpu().numpy(), bboxes[data_idx][:, 4].cpu().numpy()\n",
    "        # print(f'[{batch_idx}/{data_idx}][0]  {bbox[:10, ...]=}')\n",
    "        # bbox = np.array([box for box in bbox.numpy() if all(box > 0)])\n",
    "        # print(f'[{batch_idx}/{data_idx}][1]  {bbox[:10, ...]=}')\n",
    "        img = (np.transpose(img, (1,2,0))*128+127.5).astype(np.uint8)\n",
    "        bbox[:, 0] -= bbox[:, 2]/2\n",
    "        bbox[:, 1] -= bbox[:, 3]/2\n",
    "        bbox[:, 2] += bbox[:, 0]\n",
    "        bbox[:, 3] += bbox[:, 1]\n",
    "        bbox = bbox.astype(np.uint32)\n",
    "        img = PILDraw(font_file=r'./tools/viz/simhei.ttf').rectangle(Image.fromarray(img), bbox, [{'cls': int(l)} for l in label])\n",
    "        imgs_.append(img)\n",
    "    imgs_ = np.concatenate(imgs_, axis=1)\n",
    "    Image.fromarray(imgs_).save(f'./test/tmp_data/{batch_idx:08}-{data_idx:08}.jpg')\n",
    "    if batch_idx % 5 == 0:\n",
    "        print(f'[{batch_idx}]  {imgs_.shape=}')\n",
    "    # plt.cla()\n",
    "    # plt.imshow(imgs_);\n",
    "    # display.clear_output(wait=True)\n",
    "    # plt.pause(0.001)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dali dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "os.chdir(os.path.dirname(os.path.abspath('.')))\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import dataset.coco_dataset_yolox_v1 as DS\n",
    "import configs.config_instance as Config\n",
    "from tools.viz.pil_draw import PILDraw\n",
    "from IPython import display\n",
    "import cv2\n",
    "import dataset.coco_dataset_yolox_dali_v1 as DSD\n",
    "\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "# %matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib as ilib\n",
    "ilib.reload(DSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = Config.TRAINCONFIGS  # EVALCONFIGS, TRAINCONFIGS\n",
    "# ann_file = os.path.join(cfgs.dataset_root, 'annotations', f'person_keypoints_{cfgs.dataset_set_name}.json')\n",
    "ann_file = os.path.join(cfgs.dataset_root, 'annotations', f'instances_val2017.json')\n",
    "coco = COCO(ann_file)\n",
    "img_ids = coco.getImgIds()\n",
    "print(f'load train ann-file: {ann_file} / {len(img_ids)}')\n",
    "\n",
    "\n",
    "ds = DSD.COCODetDataset(\n",
    "    root_dir=cfgs.dataset_root, \n",
    "    input_shape=cfgs.input_shape, \n",
    "    mosaic=cfgs.mosaic, mosaic_prob=cfgs.mosaic_prob, \n",
    "    mixup=cfgs.mixup, mixup_prob=cfgs.mixup_prob, \n",
    "    train=True, \n",
    "    set_name='val2017', \n",
    "    coco=coco,\n",
    "    special_aug_ratio = 0.7,\n",
    "    mixup_alpha=0.5, mixup_beta=0.5, \n",
    "    jitter=True, random_scale=cfgs.random_scale,\n",
    "    ex_kpts=0,\n",
    "    class_list=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_gpus = 1\n",
    "num_threads = 8\n",
    "epochs = 2\n",
    "\n",
    "pii = DSD.create_dataloder(ds, resize=640, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    print('tttt', len(pii))\n",
    "    for i, data in enumerate(pii):\n",
    "        imgs = data[0][\"data\"]#.cpu().numpy()\n",
    "        labels = data[0][\"label\"]#.cpu().numpy()\n",
    "        num_objs = data[0][\"num_objs\"]#.cpu().numpy()\n",
    "        # print(\"epoch: {}, iter {}\".format(e, i), imgs.shape, labels.shape)\n",
    "        print(f'epoch: {e}, iter {i}  {imgs.shape=}  {imgs.min()=} / {imgs.max()=}  {imgs.device=}  {labels.shape=}  {labels.device=}  {num_objs.shape=}')\n",
    "        \n",
    "        imgs = imgs.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        \n",
    "        for img_idx in range(imgs.shape[0]):\n",
    "            img_ = Image.fromarray(imgs[img_idx, ...])\n",
    "            num_objs_ = num_objs[img_idx][0]\n",
    "            bboxes = labels[img_idx, :, 0:4][0:num_objs_, :].astype(np.int32)\n",
    "            img = PILDraw().rectangle(img_, bboxes, [{'cls': int(l)} for l in labels[img_idx, :, 4]])\n",
    "            cv2.imwrite(f'./dataset/dali_test/{e}-{i}-{img_idx}.jpg', np.array(img))\n",
    "\n",
    "        if i > 10:\n",
    "            # sys.exit(0)\n",
    "            break\n",
    "        \n",
    "    pii.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aug paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(shape1, shape2):\n",
    "    w,h = shape1\n",
    "    wi, hi = shape2\n",
    "    if w/wi > h/hi:\n",
    "        scale = h/hi\n",
    "    else:\n",
    "        scale = w/wi\n",
    "    \n",
    "    nw, nh = wi * scale, hi * scale\n",
    "\n",
    "    return nw, nh, scale\n",
    "\n",
    "def get_pad(shape1, shape2, shift=True):\n",
    "    l, t, r, b = 0, 0, 0, 0\n",
    "    l = r = int((shape1[0] - shape2[0]) /2)\n",
    "    t = b = int((shape1[1] - shape2[1]) /2)\n",
    "    return l, t, r, b\n",
    "\n",
    "def paste(img1, img2, bboxes=None, kpts=None):\n",
    "    new_shape = get_shape(img1.size, img2.size)\n",
    "    new_img = img2.resize((int(new_shape[0]), int(new_shape[1])))\n",
    "    l, t, r, b = get_pad(img1.size, new_img.size)\n",
    "    img1.paste(new_img, (l, t))\n",
    "    if bboxes is not None:\n",
    "        bboxes *= new_shape[2]\n",
    "        bboxes[:, (0, 2)] += l\n",
    "        bboxes[:, (1, 3)] += t\n",
    "    if kpts is not None:\n",
    "        kpts *= new_shape[2]\n",
    "        kpts[:, 0] += l\n",
    "        kpts[:, 1] += t\n",
    "    return img1, bboxes, kpts\n",
    "\n",
    "\n",
    "def draw(img, bboxes, kpts, color=(0,0,255)):\n",
    "    img_ = np.array(img)\n",
    "    for bbox in bboxes:\n",
    "        img_ = cv2.rectangle(img_, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n",
    "    for kpt in kpts:\n",
    "        img_ = cv2.circle(img_, (int(kpt[0]), int(kpt[1])), 5, color, 5)\n",
    "    return img_\n",
    "\n",
    "\n",
    "def adjust_bboxs_kpts(shape, bboxes, kpts=None, invalid=True):\n",
    "    h, w = shape\n",
    "    \n",
    "    if invalid:\n",
    "        box_w = bboxes[:, 2] - bboxes[:, 0]\n",
    "        box_h = bboxes[:, 3] - bboxes[:, 1]\n",
    "        bboxes = bboxes[np.logical_and(box_w>1, box_h>1)] # discard invalid box\n",
    "    \n",
    "    bboxes[bboxes < 0] = 0\n",
    "    bboxes[:, (0,2)][bboxes[:, (0,2)] > w] = w\n",
    "    bboxes[:, (0,2)][bboxes[:, (1,3)] > h] = h\n",
    "    \n",
    "    if kpts is not None:\n",
    "        kpts[kpts < 0] = 0\n",
    "        kpts[:, 0][kpts[:, 0] > w] = w\n",
    "        kpts[:, 1][kpts[:, 1] > h] = h\n",
    "    \n",
    "    return bboxes, kpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "shape = (1500,1000)\n",
    "img = Image.new('RGB', shape, (0,0,0))\n",
    "img1 = r'/data/ylw/code/yolo/yolox-pytorch/yolox-v1/test/tmp__/mosaic.jpg'\n",
    "img1 = Image.open(img1)\n",
    "\n",
    "bboxes = np.array([[10, 10, 60, 60], [100, 100, 500, 500], [-100, -100, -10, -10], [-100, -100, 10, 10], [200, 200, 2100, 2100], [800, 800, 1200, 1200]], dtype=np.float64)\n",
    "kpts = np.array([[60, 60], [90, 90], [220, 220]], dtype=np.float64)\n",
    "img1 = Image.fromarray(draw(img1, bboxes, kpts, color=(0,0,255)))\n",
    "\n",
    "print(f'{bboxes=}  {kpts=}')\n",
    "bboxes, kpts = adjust_bboxs_kpts(np.array(img1).shape[0:2], bboxes, kpts)\n",
    "img1 = Image.fromarray(draw(img1, bboxes, kpts, color=(0,255,255)))\n",
    "print(f'{bboxes=}  {kpts=}')\n",
    "\n",
    "img, bboxes, kpts = paste(img, img1, bboxes, kpts)\n",
    "img = Image.fromarray(draw(img, bboxes, kpts, color=(255,0,255)))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aug  rotate_center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_center(img, degree, scale, bboxes=None, kpts=None):\n",
    "    '''\n",
    "    img: opencv type\n",
    "    '''\n",
    "    rows, cols, channel = img.shape\n",
    "    # 参数：旋转中心 旋转度数 scale\n",
    "    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), degree, scale)\n",
    "    # 参数：原始图像 旋转参数 元素图像宽高\n",
    "    rotated = cv2.warpAffine(img, M, (cols, rows))\n",
    "    bboxes_ = bboxes\n",
    "    if bboxes_ is not None:\n",
    "        left, top, right, bottom = bboxes[:, 0:1], bboxes[:, 1:2], bboxes[:, 2:3], bboxes[:, 3:4]\n",
    "        lt = np.concatenate([left, top], axis=1)\n",
    "        rt = np.concatenate([right, top], axis=1)\n",
    "        lb = np.concatenate([left, bottom], axis=1)\n",
    "        rb = np.concatenate([right, bottom], axis=1)\n",
    "        r_bboxes = []\n",
    "        for ps in [lt, rt, lb, rb]:\n",
    "            xy1 = ps\n",
    "            xy1 = np.column_stack((xy1.copy(), np.ones(xy1.shape[0]).T))\n",
    "            xy1 = np.transpose(xy1, (1, 0))\n",
    "            xy1 = np.dot(M, xy1)\n",
    "            xy1 = np.transpose(xy1, (1, 0))\n",
    "            r_bboxes.append(xy1)\n",
    "        r_bboxes = np.concatenate(r_bboxes, axis=1).reshape((-1, 4, 2))\n",
    "        l, r, t, b = r_bboxes[:, :, 0].min(axis=1), r_bboxes[:, :, 0].max(axis=1), r_bboxes[:, :, 1].min(axis=1), r_bboxes[:, :, 1].max(axis=1)\n",
    "        bboxes_ = np.concatenate([l[:, np.newaxis], t[:, np.newaxis], r[:, np.newaxis], b[:, np.newaxis]], axis=1)\n",
    "\n",
    "    kpts_ = None\n",
    "    if kpts is not None:\n",
    "        kpts_ = kpts.copy()\n",
    "        kpts_mode = kpts.shape[1]\n",
    "        kpts__ = kpts_[:, 0:2].copy() if kpts_mode == 3 else kpts_\n",
    "        kpts__ = np.column_stack((kpts__, np.ones(kpts_.shape[0]).T))\n",
    "        kpts__ = np.transpose(kpts__, (1, 0))\n",
    "        kpts__ = np.dot(M, kpts__)\n",
    "        kpts__ = np.transpose(kpts__, (1, 0))\n",
    "        kpts_[:, 0:2] = kpts__\n",
    "        kpts_ = kpts__\n",
    "    return rotated, bboxes_, kpts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate_center\n",
    "img_file = r'/data/ylw/code/yolo/yolox-pytorch/yolox-v1/img/001.jpg'\n",
    "bbox = np.array([100, 300, 200, 500, 150, 200, 300, 500]).reshape(-1, 4)\n",
    "kpts = np.array([[30,30,1], [50,50,0], [100,100,2]])\n",
    "img = cv2.imread(img_file)\n",
    "print(f'{img.shape=}  {bbox=}  {kpts=}')\n",
    "\n",
    "\n",
    "\n",
    "degree = 90\n",
    "scale = 1\n",
    "img_, bboxes_, kpts_ = rotate_center(img, degree, scale, bboxes=bbox, kpts=kpts)\n",
    "print(f'{img_.shape=}  {bboxes_=}  {kpts_=}')\n",
    "\n",
    "img_ = draw(img_, bboxes_, kpts=kpts_, color=(0, 0, 255))\n",
    "img_ = draw(img_, bbox, kpts=kpts, color=(255, 0, 0))\n",
    "plt.imshow(img_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rotate_center_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import fabs, cos, sin, radians\n",
    "\n",
    "img_file = r'/data/ylw/code/yolo/yolox-pytorch/yolox-v1/img/001.jpg'\n",
    "bbox = np.array([100, 300, 200, 500, 150, 200, 300, 500]).reshape(-1, 4)\n",
    "kpts = np.array([[30,30,1], [50,50,0], [100,100,2]])\n",
    "img = cv2.imread(img_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rotate_keep(degree, width, height):\n",
    "    \"\"\"\n",
    "    无损图像的旋转\n",
    "    @param degree:\n",
    "    @param width:\n",
    "    @param height:\n",
    "    @return: 旋转矩阵，新的宽高\n",
    "    \"\"\"\n",
    "    height_new = int(width * fabs(sin(radians(degree))) + height * fabs(cos(radians(degree))))\n",
    "    width_new = int(height * fabs(sin(radians(degree))) + width * fabs(cos(radians(degree))))\n",
    "    mat_rotation = cv2.getRotationMatrix2D((width / 2, height / 2), degree, 1)\n",
    "    mat_rotation[0, 2] += (width_new - width) / 2\n",
    "    mat_rotation[1, 2] += (height_new - height) / 2\n",
    "    return mat_rotation, width_new, height_new\n",
    "\n",
    "angle = 45\n",
    "height, width = img.shape[0:2]\n",
    "M, width_new, height_new = rotate_keep(45, width, height)\n",
    "image_new = cv2.warpAffine(img, M, (width_new, height_new))\n",
    "plt.imshow(image_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_center(img, degree, scale, bboxes=None, kpts=None):\n",
    "    '''\n",
    "    img: opencv type\n",
    "    '''\n",
    "    rows, cols, channel = img.shape\n",
    "    # 参数：旋转中心 旋转度数 scale\n",
    "    # M = cv2.getRotationMatrix2D((cols / 2, rows / 2), degree, scale)\n",
    "    # 参数：原始图像 旋转参数 元素图像宽高\n",
    "    # rotated = cv2.warpAffine(img, M, (cols, rows))\n",
    "    M, width_new, height_new = rotate_keep(degree, cols, rows)\n",
    "    rotated = cv2.warpAffine(img, M, (width_new, height_new))\n",
    "    bboxes_ = bboxes\n",
    "    if bboxes_ is not None:\n",
    "        left, top, right, bottom = bboxes[:, 0:1], bboxes[:, 1:2], bboxes[:, 2:3], bboxes[:, 3:4]\n",
    "        lt = np.concatenate([left, top], axis=1)\n",
    "        rt = np.concatenate([right, top], axis=1)\n",
    "        lb = np.concatenate([left, bottom], axis=1)\n",
    "        rb = np.concatenate([right, bottom], axis=1)\n",
    "        r_bboxes = []\n",
    "        for ps in [lt, rt, lb, rb]:\n",
    "            xy1 = ps\n",
    "            xy1 = np.column_stack((xy1.copy(), np.ones(xy1.shape[0]).T))\n",
    "            xy1 = np.transpose(xy1, (1, 0))\n",
    "            xy1 = np.dot(M, xy1)\n",
    "            xy1 = np.transpose(xy1, (1, 0))\n",
    "            r_bboxes.append(xy1)\n",
    "        r_bboxes = np.concatenate(r_bboxes, axis=1).reshape((-1, 4, 2))\n",
    "        l, r, t, b = r_bboxes[:, :, 0].min(axis=1), r_bboxes[:, :, 0].max(axis=1), r_bboxes[:, :, 1].min(axis=1), r_bboxes[:, :, 1].max(axis=1)\n",
    "        bboxes_ = np.concatenate([l[:, np.newaxis], t[:, np.newaxis], r[:, np.newaxis], b[:, np.newaxis]], axis=1)\n",
    "\n",
    "    kpts_ = None\n",
    "    if kpts is not None:\n",
    "        kpts_ = kpts.copy()\n",
    "        kpts_mode = kpts.shape[1]\n",
    "        kpts__ = kpts_[:, 0:2].copy() if kpts_mode == 3 else kpts_\n",
    "        kpts__ = np.column_stack((kpts__, np.ones(kpts_.shape[0]).T))\n",
    "        kpts__ = np.transpose(kpts__, (1, 0))\n",
    "        kpts__ = np.dot(M, kpts__)\n",
    "        kpts__ = np.transpose(kpts__, (1, 0))\n",
    "        kpts_[:, 0:2] = kpts__\n",
    "        kpts_ = kpts__\n",
    "    return rotated, bboxes_, kpts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate_center\n",
    "img_file = r'/data/ylw/code/yolo/yolox-pytorch/yolox-v1/img/001.jpg'\n",
    "bbox = np.array([100, 300, 200, 500, 150, 200, 300, 500]).reshape(-1, 4)\n",
    "kpts = np.array([[30,30,1], [50,50,0], [100,100,2]])\n",
    "img = cv2.imread(img_file)\n",
    "print(f'{img.shape=}  {bbox=}  {kpts=}')\n",
    "img_ = draw(img.copy(), bbox, kpts=kpts, color=(255, 0, 0))\n",
    "plt.imshow(img_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "degree = 90\n",
    "scale = 1\n",
    "img_, bboxes_, kpts_ = rotate_center(img, degree, scale, bboxes=bbox, kpts=kpts)\n",
    "print(f'{img_.shape=}  {bboxes_=}  {kpts_=}')\n",
    "\n",
    "img_ = draw(img_, bboxes_, kpts=kpts_, color=(0, 0, 255))\n",
    "plt.imshow(img_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(r'/data/ylw/code/yolo/yolox-pytorch/yolox-v1/tools')\n",
    "from augs import augs as Augs\n",
    "from viz import pil_draw as PILDraw\n",
    "\n",
    "import importlib\n",
    "importlib.reload(PILDraw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate_center\n",
    "img_file = r'/data/ylw/code/yolo/yolox-pytorch/yolox-v1/img/001.jpg'\n",
    "bbox = np.array([100., 300, 200, 500, 150, 200, 300, 500]).reshape(-1, 4)\n",
    "kpts = np.array([[30.,30,1], [50,50,0], [100,100,2]])\n",
    "img = cv2.imread(img_file)\n",
    "print(f'{img.shape=}  {bbox=}  {kpts=}')\n",
    "img_ = draw(img.copy(), bbox, kpts=kpts, color=(255, 0, 0))\n",
    "plt.imshow(img_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (640, 640)\n",
    "print(f'{bbox=}')\n",
    "img_, bboxes_, points_ = Augs.resize_image(img, size, letterbox_image=False, bboxes=bbox, points=kpts)\n",
    "print(f'{bboxes_=}  {points_.shape=}')\n",
    "img_ = PILDraw.PILDraw().rectangle(img_, bboxes_, labels=[{'cls':i} for i in range(bboxes_.shape[0])], fille=None, outline=None)\n",
    "img_ = PILDraw.PILDraw().points(img_, points_, num_points=1, labels=None)\n",
    "plt.imshow(img_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_file = r'/data/Public/small_algo/detection_eleven/det11_dataset_2.0/coco_labels/test.json'\n",
    "coco = COCO(ann_file)\n",
    "img_ids = coco.getImgIds()\n",
    "print(f'{len(img_ids)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = r'/data/Public/small_algo/detection_eleven/det11_dataset_2.0'\n",
    "\n",
    "image_index = img_ids[0]\n",
    "\n",
    "image_info = coco.loadImgs(image_index)[0]\n",
    "print(f'{image_info=}')\n",
    "\n",
    "img_file = os.path.join(img_root, image_info['file_name'])\n",
    "img = cv2.imread(img_file)\n",
    "\n",
    "anno_ids = coco.getAnnIds(imgIds=image_info['id'], iscrowd=False)\n",
    "annos = coco.loadAnns(anno_ids)\n",
    "print(f'{annos=}')\n",
    "\n",
    "bboxes = np.array([ann['bbox'] for ann in annos if ann['category_id'] == 0]).reshape((-1, 4))\n",
    "# bboxes[:, (0, 1)] -= bboxes[:, (2, 3)]/2\n",
    "bboxes[:, (2, 3)] += bboxes[:, (0, 1)]\n",
    "\n",
    "def draw(img, bboxes=None, kpts=None, color=(0,0,255)):\n",
    "    img_ = np.array(img)\n",
    "    if bboxes is not None:\n",
    "        for bbox in bboxes:\n",
    "            img_ = cv2.rectangle(img_, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 5)\n",
    "    if kpts is not None:\n",
    "        for kpt in kpts:\n",
    "            img_ = cv2.circle(img_, (int(kpt[0]), int(kpt[1])), 5, color, 5)\n",
    "    return img_\n",
    "\n",
    "img = draw(img, bboxes)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test albument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "os.chdir(os.path.dirname(os.path.abspath('.')))\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# import dataset.coco_dataset_yolox_v1 as DS\n",
    "import dataset.coco_dataset_yolox_v2 as DS\n",
    "import configs.config_instance as Config\n",
    "from tools.viz.pil_draw import PILDraw\n",
    "from IPython import display\n",
    "import cv2\n",
    "\n",
    "# %matplotlib notebook\n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "# %matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = Config.TRAINCONFIGS  # EVALCONFIGS, TRAINCONFIGS\n",
    "cfgs.dataset_root = r'/data/ylw/dataset/voc2coco'\n",
    "# ann_file = os.path.join(cfgs.dataset_root, 'annotations', f'person_keypoints_{cfgs.dataset_set_name}.json')\n",
    "ann_file = os.path.join(cfgs.dataset_root, 'annotations', f'instances_val.json')\n",
    "coco = COCO(ann_file)\n",
    "img_ids = coco.getImgIds()\n",
    "print(f'load train ann-file: {ann_file} / {len(img_ids)}')\n",
    "\n",
    "\n",
    "ds = DS.COCODetDataset(\n",
    "    root_dir=cfgs.dataset_root, \n",
    "    input_shape=cfgs.input_shape, \n",
    "    mosaic=cfgs.mosaic, mosaic_prob=cfgs.mosaic_prob, \n",
    "    mixup=cfgs.mixup, mixup_prob=cfgs.mixup_prob, \n",
    "    train=True, \n",
    "    set_name='val', \n",
    "    coco=coco,\n",
    "    special_aug_ratio = 0.7,\n",
    "    mixup_alpha=0.5, mixup_beta=0.5, \n",
    "    jitter=True, random_scale=cfgs.random_scale,\n",
    "    ex_kpts=0,\n",
    "    class_list=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    ds,\n",
    "    shuffle = False,\n",
    "    batch_size = 2,\n",
    "    num_workers = cfgs.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=ds.yolox_dataset_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch_data in enumerate(dl):\n",
    "    imgs, bboxes = batch_data[0], batch_data[1]\n",
    "    print(f'{imgs.shape=}  {len(bboxes)=}  {bboxes[0].shape=}')\n",
    "    # if batch_idx > 10:\n",
    "    #     break\n",
    "    imgs_ = []\n",
    "    for data_idx in range(batch_data[0].shape[0]):\n",
    "        img, bbox, label = imgs[data_idx, ...].cpu().numpy(), bboxes[data_idx][:, 0:4].cpu().numpy(), bboxes[data_idx][:, 4].cpu().numpy()\n",
    "        # print(f'[{batch_idx}/{data_idx}][0]  {bbox[:10, ...]=}')\n",
    "        # bbox = np.array([box for box in bbox.numpy() if all(box > 0)])\n",
    "        # print(f'[{batch_idx}/{data_idx}][1]  {bbox[:10, ...]=}')\n",
    "        img = (np.transpose(img, (1,2,0))*128+127.5).astype(np.uint8)\n",
    "        bbox[:, 0] -= bbox[:, 2]/2\n",
    "        bbox[:, 1] -= bbox[:, 3]/2\n",
    "        bbox[:, 2] += bbox[:, 0]\n",
    "        bbox[:, 3] += bbox[:, 1]\n",
    "        bbox = bbox.astype(np.uint32)\n",
    "        img = PILDraw(font_file=r'./tools/viz/simhei.ttf').rectangle(Image.fromarray(img), bbox, [{'cls': int(l)} for l in label])\n",
    "        imgs_.append(img)\n",
    "    imgs_ = np.concatenate(imgs_, axis=1)\n",
    "    Image.fromarray(imgs_).save(f'./test/tmp_data/{batch_idx:08}-{data_idx:08}.jpg')\n",
    "    if batch_idx % 5 == 0:\n",
    "        print(f'[{batch_idx}]  {imgs_.shape=}')\n",
    "    # plt.cla()\n",
    "    # plt.imshow(imgs_);\n",
    "    # display.clear_output(wait=True)\n",
    "    # plt.pause(0.001)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
